# LanguageTranslationTransformer-LTT-
# ğŸ§  Custom Language Translation Transformer (Englishâ€“Telugu)

This project implements a Transformer-based model from scratch. It translates sentences from **English to Telugu**, demonstrating deep understanding of sequence-to-sequence learning, self-attention mechanisms, and positional encodings.
This Model was Trained with limited resources(Google Colab GPU) for 10 epochs with 40,000 data. still the performance is good.
SOURCE: I am not a very good a student.
PREDICTED: à°¨à±‡à°¨à± à°šà°¾à°²à°¾ à°®à°‚à°šà°¿ à°µà°¿à°¦à±à°¯à°¾à°°à±à°¥à°¿à°¨à°¿ à°•à°¾à°¦à± .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 

---

## ğŸš€ Features

- âœ… Transformer architecture fully implemented from scratch in PyTorch
- âœ… Encoderâ€“Decoder architecture with multi-head attention
- âœ… positional encoding
- âœ… Custom data preprocessing and batching pipeline
- âœ… Trained on parallel Englishâ€“Telugu dataset
- âœ… Supports inference from both raw text or test set index
- âœ… Model export/load using `torch.save()` and `load_state_dict()`
---

